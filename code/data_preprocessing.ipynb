{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca59e1ea-d688-49b9-ae69-a966c34ef96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_id': 'Q1', 'question': 'how are glacier caves formed?', 'document_title': 'Glacier cave', 'answer': 'A partly submerged glacier cave on Perito Moreno Glacier .', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import gc\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dataset = load_dataset(\"wiki_qa\")\n",
    "\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4daf95cc-b3e6-4080-983c-683604c12144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are glacier caves formed?\n",
      "2118\n"
     ]
    }
   ],
   "source": [
    "query_list = {}\n",
    "for train_data in dataset['train']:\n",
    "    if train_data['question_id'] not in query_list:\n",
    "        query_list[train_data['question_id']] = train_data['question']\n",
    "for train_data in dataset['validation']:\n",
    "    if train_data['question_id'] not in query_list:\n",
    "        query_list[train_data['question_id']] = train_data['question']\n",
    "    \n",
    "print(query_list['Q1'])\n",
    "print(len(query_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9676e702-2a21-45b9-9f5b-ef15b9b0872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are glacier caves formed?\n",
      "2414\n"
     ]
    }
   ],
   "source": [
    "for q_index, question in query_list.items():\n",
    "    print(question)\n",
    "    break\n",
    "print(len(query_list.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f81b47e7-bddb-49ac-8303-a2dddd2741b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_data(data):\n",
    "    \"\"\"Removes all the unnecessary patterns and cleans the data to get a good sentence\"\"\"\n",
    "    repl='' #String for replacement\n",
    "    \n",
    "    #removing all open brackets\n",
    "    data=re.sub('\\(', repl, data)\n",
    "    \n",
    "    #removing all closed brackets\n",
    "    data=re.sub('\\)', repl, data)\n",
    "    \n",
    "    #Removing all the headings in data\n",
    "    for pattern in set(re.findall(\"=.*=\",data)):\n",
    "        data=re.sub(pattern, repl, data)\n",
    "    \n",
    "    #Removing unknown words in data\n",
    "    for pattern in set(re.findall(\"<unk>\",data)):\n",
    "        data=re.sub(pattern,repl,data)\n",
    "    \n",
    "    #Removing all the non-alphanumerical characters\n",
    "    for pattern in set(re.findall(r\"[^\\w ]\", data)):\n",
    "        repl=''\n",
    "        if pattern=='-':\n",
    "            repl=' '\n",
    "        #Retaining period, apostrophe\n",
    "        if pattern!='.' and pattern!=\"\\'\":\n",
    "            data=re.sub(\"\\\\\"+pattern, repl, data)\n",
    "            \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66fbe499-a407-4121-9518-e7d2163702f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are glacier caves formed\n",
      "2414\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = []\n",
    "for q_index, question in query_list.items():\n",
    "    data = Clean_data(question)\n",
    "    cleaned_data.append(data)\n",
    "print(cleaned_data[0])\n",
    "print(len(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5168c275-03a0-4e59-bbaa-343b046bd9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xy_pairs(questions):\n",
    "    xy_pairs = []\n",
    "    for question in questions:\n",
    "        tokens = question.split()  # 假设使用空格进行简单分词\n",
    "        for i in range(1, len(tokens)):\n",
    "            x = \" \".join(tokens[:i])\n",
    "            y = tokens[i]\n",
    "            xy_pairs.append((x, y))\n",
    "    return xy_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13a3280e-48b1-476f-b693-daa187969e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('how', 'are'), ('how are', 'glacier'), ('how are glacier', 'caves'), ('how are glacier caves', 'formed'), ('How', 'are')]\n"
     ]
    }
   ],
   "source": [
    "xy_pairs = create_xy_pairs(cleaned_data)\n",
    "\n",
    "# 查看构建的一些（X, Y）对\n",
    "print(xy_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0dc1850d-c9d6-4584-b863-67b71ba54c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(sentences):\n",
    "    #Word tokenization\n",
    "    words=set()\n",
    "    for sent in sentences:\n",
    "        for word in str.split(sent,' '):\n",
    "            words.add(word)\n",
    "    words=list(words)\n",
    "    \n",
    "    #Adding empty string in list of words to avoid confusion while padding.\n",
    "    #Padded zeroes can be interpreted as empty strings.\n",
    "    words.insert(0,\"\")\n",
    "    return words\n",
    "vocabs = create_vocab(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3af85ffe-2415-487b-900b-3aaa5af1c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_data(sentences, words, seq_len):\n",
    "    \"\"\"Converts text data into numerical form\"\"\"\n",
    "    \n",
    "    sent_sequences=[]\n",
    "    for i in range(len(sentences)):\n",
    "        words_in_sent=str.split(sentences[i],' ')\n",
    "        for j in range(1,len(words_in_sent)):\n",
    "            if j<=(seq_len):\n",
    "                sent_sequences.append(words_in_sent[:j])\n",
    "            elif j>seq_len and j<len(words_in_sent):\n",
    "                sent_sequences.append(words_in_sent[j-seq_len:j])\n",
    "            elif j>len(words_in_sent)-seq_len:\n",
    "                sent_sequences.append(words_in_sent[j-seq_len:])\n",
    "                \n",
    "    #The above code converts the text data into the following sequences\n",
    "    #[['The', '2013'],\n",
    "    #['The', '2013', '14'],\n",
    "    #['The', '2013', '14', 'season'],\n",
    "    #['The', '2013', '14', 'season', 'was']]\n",
    "    \n",
    "    #Splitting into predictors and class_labels\n",
    "    predictors=[];class_labels=[]\n",
    "    for i in range(len(sent_sequences)):\n",
    "        predictors.append(sent_sequences[i][:-1])\n",
    "        class_labels.append(sent_sequences[i][-1])\n",
    "    \n",
    "    #Padding the predictors manually with Empty strings\n",
    "    pad_predictors=[]\n",
    "    for i in range(len(predictors)):\n",
    "        emptypad=['']*(seq_len-len(predictors[i])-1)\n",
    "        emptypad.extend(predictors[i])\n",
    "        pad_predictors.append(emptypad)\n",
    "        \n",
    "    #The following two chunks of code are useful to convert text into numeric form\n",
    "    #Dictionary with words as keys and indices as values\n",
    "    global word_ind\n",
    "    word_ind=dict()\n",
    "    for ind,word in enumerate(words):\n",
    "        word_ind[word]=ind\n",
    "    \n",
    "    #Dictionary with indices as keys and words as values\n",
    "    global ind_word\n",
    "    ind_word=dict()\n",
    "    for ind,word in enumerate(words):\n",
    "        ind_word[ind]=word\n",
    "        \n",
    "    #Convert each word into their respective index\n",
    "    for i in range(len(pad_predictors)):\n",
    "        for j in range(len(pad_predictors[i])):\n",
    "            pad_predictors[i][j]=word_ind[pad_predictors[i][j]]\n",
    "        class_labels[i]=word_ind[class_labels[i]]\n",
    "        \n",
    "    #Convert sequences to tensors\n",
    "    for i in range(len(pad_predictors)):\n",
    "        pad_predictors[i]=torch.tensor(pad_predictors[i])\n",
    "    pad_predictors=torch.stack(pad_predictors)\n",
    "    class_labels=torch.tensor(class_labels)\n",
    "     \n",
    "    return pad_predictors, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1169cc1-d656-4e63-8f72-3a7e234afa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"Base class for all neural network modules.\n",
    "       All models should subclass this class\"\"\"\n",
    "    def __init__(self,num_embeddings, embedding_dim, padding_idx, hidden_size, Dropout_p, batch_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.num_embeddings=num_embeddings\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.padding_idx=padding_idx\n",
    "        self.hidden_size=hidden_size\n",
    "        self.dropout=Dropout_p\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        #Adding Embedding Layer\n",
    "        self.Embedding=nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        #Adding LSTM Layer\n",
    "        self.lstm=nn.LSTM(embedding_dim, hidden_size, num_layers=1, batch_first=True)\n",
    "        \n",
    "        #Adding Dropout Layer\n",
    "        self.dropout=nn.Dropout(Dropout_p)\n",
    "        \n",
    "        #Adding fully connected dense Layer\n",
    "        self.FC=nn.Linear(hidden_size, num_embeddings)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initializes hiddens state tensors to zeros\"\"\"\n",
    "        \n",
    "        state_h=torch.zeros(1, batch_size, self.hidden_size)\n",
    "        state_c=torch.zeros(1, batch_size, self.hidden_size)\n",
    "        \n",
    "        return (state_h,state_c)\n",
    "        \n",
    "    def forward(self,input_sequence, state_h,state_c):\n",
    "        \n",
    "        #Applying embedding layer to input sequence\n",
    "        Embed_input=self.Embedding(input_sequence)\n",
    "        \n",
    "        #Applying LSTM layer\n",
    "        output,(state_h,state_c)=self.lstm(Embed_input, (state_h,state_c)) \n",
    "        \n",
    "        #Applying fully connected layer\n",
    "        logits=self.FC(output[:,-1,:])\n",
    "         \n",
    "        return logits,(state_h,state_c)\n",
    "    \n",
    "    def topk_sampling(self, logits, topk):\n",
    "        \"\"\"Applies softmax layer and samples an index using topk\"\"\"\n",
    "        \n",
    "        #Applying softmax layer to logits\n",
    "        logits_softmax=F.softmax(logits,dim=1)\n",
    "        values,indices=torch.topk(logits_softmax[0],k=topk)\n",
    "        choices=indices.tolist()\n",
    "        sampling=random.sample(choices,1)\n",
    "        \n",
    "        return ind_word[sampling[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f758b9a-1eec-4bc5-9c78-a9d33ad87b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(pad_predictors, class_labels, batch_size):\n",
    "    for i in range(0, len(pad_predictors), batch_size):\n",
    "        if i+batch_size<len(pad_predictors):\n",
    "            yield pad_predictors[i:i+batch_size], class_labels[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ffb0ebb-f0dd-46a7-a7a8-64f747cb04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(pad_predictors, class_labels, n_vocab, embedding_dim, padding_idx, hidden_size, Dropout_p, batch_size, lr):\n",
    "    \"\"\"Trains an LSTM Model\"\"\"\n",
    "    #Creates instance of LSTM class\n",
    "    model=LSTM(n_vocab, embedding_dim, padding_idx, hidden_size, Dropout_p, batch_size)\n",
    "    \n",
    "    #Creates instance of CrossEntropLoss class\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    #Creates instance of Adam optimizer class\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    num_epochs=100\n",
    "    for epoch in range(num_epochs):\n",
    "        state_h, state_c=model.init_hidden(batch_size)\n",
    "        \n",
    "        total_loss=0\n",
    "        for x, y in get_batch(pad_predictors, class_labels, batch_size):\n",
    "            #sets model in training mode\n",
    "            model.train()\n",
    "            \n",
    "            state_h=state_h.detach()\n",
    "            state_c=state_c.detach()\n",
    "            \n",
    "            logits,(state_h,state_c)=model(x, state_h, state_c)\n",
    "           \n",
    "            #compute loss\n",
    "            loss = criterion(logits, y)\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=len(x)*loss_value\n",
    "\n",
    "            #Sets the gradients of all the optimized tensors to zero\n",
    "            model.zero_grad()\n",
    "\n",
    "            #computes dloss/dx and assigns gradient for every parameter\n",
    "            loss.backward()\n",
    "\n",
    "            #Clips the gradient norm to avoid exploding gradient problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "            #Performs a single optimization step (parameter update).\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_loss/=len(pad_predictors)\n",
    "            \n",
    "        print(\"Epoch [{}/{}] Loss: {}, perplexity: {}\".\n",
    "              format(epoch+1, num_epochs, total_loss, np.exp(total_loss)))\n",
    "        \n",
    "        gen_text=generate(model, init='How are', sent_len=10, topk=3)\n",
    "        print(\"Text generated after epoch\", epoch,\":\")\n",
    "        print(\"\\n\",end='')\n",
    "        print(gen_text)\n",
    "        print('\\n',end='')\n",
    "        \n",
    "    return model, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b706c806-28b6-432f-ae81-f1617b46f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, init, sent_len, topk):\n",
    "    \"\"\"Generates sentences from the model\"\"\"\n",
    "\n",
    "    sentence=init\n",
    "    for k in range(sent_len):\n",
    "        #sets model in evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        #sets the length of sentence to seq_len\n",
    "        input_indices=[]\n",
    "        for word in str.split(sentence,\" \"):\n",
    "            input_indices.append(word_ind[word])\n",
    "        if len(input_indices)<seq_len-1:\n",
    "            input_tensor=[0]*(seq_len-len(input_indices)-1)\n",
    "            input_tensor.extend(input_indices)\n",
    "        else:\n",
    "            input_tensor=input_indices[-seq_len+1:]\n",
    "            \n",
    "        #Initiates hidden state and cell state tensors to zeros\n",
    "        state_h, state_c=model.init_hidden(len(input_tensor))\n",
    "        \n",
    "        input_tensor=torch.stack([torch.tensor(input_tensor)])\n",
    "        out,(state_h,state_c)=model(input_tensor.transpose(0,1),state_h, state_c)\n",
    "        \n",
    "        #Samples a word from topk words\n",
    "        word=model.topk_sampling(out, topk)\n",
    "        \n",
    "        if word!='' and word!=str.split(sentence,' ')[-1]:\n",
    "            sentence=sentence+\" \"+word\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6be34249-81ef-48fd-a5ea-ea086a477b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # train=load_data(\"../input/wikitext2-data/train.txt\")\n",
    "    # data=train[:]\n",
    "    # data=Clean_data(data)\n",
    "    # sentences, words=split_data(data, num_sentences=25000)\n",
    "        \n",
    "    pad_predictors, class_labels=Convert_data(cleaned_data, vocabs, seq_len)\n",
    "    \n",
    "    print(\"Number of input sequences :\",len(pad_predictors))\n",
    "    \n",
    "    model, loss=train_model(pad_predictors, class_labels, n_vocab=len(vocabs), embedding_dim=100,\n",
    "                padding_idx=0, hidden_size=128, Dropout_p=0.1, batch_size=64, lr=0.001)\n",
    "    \n",
    "    generated_sentence=generate(model, init='The', sent_len=10, topk=5)\n",
    "    \n",
    "    #save the model\n",
    "    torch.save(model,\"./Wiki_Model.pt\")\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4fc5eba-e605-42ce-b7bd-62fa6b7a85eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input sequences : 14341\n",
      "Epoch [1/100] Loss: 6.114992013378926, perplexity: 452.5924326724992\n",
      "Text generated after epoch 0 :\n",
      "\n",
      "How are of a are of the cellular in the name was\n",
      "\n",
      "Epoch [2/100] Loss: 5.133280217369085, perplexity: 169.57244122360714\n",
      "Text generated after epoch 1 :\n",
      "\n",
      "How are in a are the an name the song\n",
      "\n",
      "Epoch [3/100] Loss: 4.814148920648526, perplexity: 123.2418790447528\n",
      "Text generated after epoch 2 :\n",
      "\n",
      "How are in the many in a name people the slugs of\n",
      "\n",
      "Epoch [4/100] Loss: 4.53378324945571, perplexity: 93.11015452131956\n",
      "Text generated after epoch 3 :\n",
      "\n",
      "How are a the much on slugs first is a is\n",
      "\n",
      "Epoch [5/100] Loss: 4.261363392901648, perplexity: 70.90659112522552\n",
      "Text generated after epoch 4 :\n",
      "\n",
      "How are the in much there song the What what liberty song\n",
      "\n",
      "Epoch [6/100] Loss: 3.9963730440405105, perplexity: 54.40048362786634\n",
      "Text generated after epoch 5 :\n",
      "\n",
      "How are a the much the magnetic name does first\n",
      "\n",
      "Epoch [7/100] Loss: 3.7313323148947264, perplexity: 41.73467486858968\n",
      "Text generated after epoch 6 :\n",
      "\n",
      "How are the in You there name harry anterior when are potter\n",
      "\n",
      "Epoch [8/100] Loss: 3.488733278725006, perplexity: 32.74444334153221\n",
      "Text generated after epoch 7 :\n",
      "\n",
      "How are in the You the a most Find most bomb\n",
      "\n",
      "Epoch [9/100] Loss: 3.2483129818547045, perplexity: 25.746867825193668\n",
      "Text generated after epoch 8 :\n",
      "\n",
      "How are of in You the dukes a Find\n",
      "\n",
      "Epoch [10/100] Loss: 3.0167618952288917, perplexity: 20.42504604786361\n",
      "Text generated after epoch 9 :\n",
      "\n",
      "How are the in You in supreme the visual fifa salmon most\n",
      "\n",
      "Epoch [11/100] Loss: 2.8055463109536114, perplexity: 16.536107295410147\n",
      "Text generated after epoch 10 :\n",
      "\n",
      "How are of the Illinoensis in France most thirds fifa in\n",
      "\n",
      "Epoch [12/100] Loss: 2.6069826429736938, perplexity: 13.558079503869488\n",
      "Text generated after epoch 11 :\n",
      "\n",
      "How are in on many in Call deck states Call group the\n",
      "\n",
      "Epoch [13/100] Loss: 2.4291612287169846, perplexity: 11.349358570487086\n",
      "Text generated after epoch 12 :\n",
      "\n",
      "How are the and assistant the most means games lazy harmful vectors\n",
      "\n",
      "Epoch [14/100] Loss: 2.2761880704261177, perplexity: 9.739483333700083\n",
      "Text generated after epoch 13 :\n",
      "\n",
      "How are the in many those rye Call states of the did\n",
      "\n",
      "Epoch [15/100] Loss: 2.1414960926774906, perplexity: 8.51216309214808\n",
      "Text generated after epoch 14 :\n",
      "\n",
      "How are the and assistant studying center means airports captain of by\n",
      "\n",
      "Epoch [16/100] Loss: 2.020126112949964, perplexity: 7.539275674006167\n",
      "Text generated after epoch 15 :\n",
      "\n",
      "How are the and Illinoensis studying US Resolves commonly captain\n",
      "\n",
      "Epoch [17/100] Loss: 1.9066039563455588, perplexity: 6.73019391219\n",
      "Text generated after epoch 16 :\n",
      "\n",
      "How are in the Illinoensis those european US smackdown Where in\n",
      "\n",
      "Epoch [18/100] Loss: 1.804905001849994, perplexity: 6.079393889914574\n",
      "Text generated after epoch 17 :\n",
      "\n",
      "How are in the assistant those Call campaign games of is\n",
      "\n",
      "Epoch [19/100] Loss: 1.7156025074107004, perplexity: 5.5600244593178445\n",
      "Text generated after epoch 18 :\n",
      "\n",
      "How are the on Illinoensis rouladen US hardcore specific madden 75\n",
      "\n",
      "Epoch [20/100] Loss: 1.6319154186500149, perplexity: 5.113660143961375\n",
      "Text generated after epoch 19 :\n",
      "\n",
      "How are in on of rouladen european hardcore windows cut are the\n",
      "\n",
      "Epoch [21/100] Loss: 1.5619417635701738, perplexity: 4.768070728116955\n",
      "Text generated after epoch 20 :\n",
      "\n",
      "How are in the of those european rye medicine of in a\n",
      "\n",
      "Epoch [22/100] Loss: 1.5025950762261473, perplexity: 4.493334498949516\n",
      "Text generated after epoch 21 :\n",
      "\n",
      "How are the on from rouladen center deck goat's jews like\n",
      "\n",
      "Epoch [23/100] Loss: 1.4432971987063627, perplexity: 4.234635257708674\n",
      "Text generated after epoch 22 :\n",
      "\n",
      "How are the on is specific 1967 wood university mechanism blue modulated\n",
      "\n",
      "Epoch [24/100] Loss: 1.402004401010995, perplexity: 4.063336365336605\n",
      "Text generated after epoch 23 :\n",
      "\n",
      "How are the in of specific center Texas medicine griffith like is\n",
      "\n",
      "Epoch [25/100] Loss: 1.3610187815402925, perplexity: 3.9001646942556616\n",
      "Text generated after epoch 24 :\n",
      "\n",
      "How are in the affect specific wayne canadian the make alighieri\n",
      "\n",
      "Epoch [26/100] Loss: 1.3228990168977455, perplexity: 3.75428936471708\n",
      "Text generated after epoch 25 :\n",
      "\n",
      "How are the in of specific canadian european windows make war in\n",
      "\n",
      "Epoch [27/100] Loss: 1.2750457575876304, perplexity: 3.5788651665914775\n",
      "Text generated after epoch 26 :\n",
      "\n",
      "How are the and is specific western driving kj jose economic athena\n",
      "\n",
      "Epoch [28/100] Loss: 1.2381603577988025, perplexity: 3.4492622162055118\n",
      "Text generated after epoch 27 :\n",
      "\n",
      "How are in the is specific n.m. western Montargis scout BRANDO information\n",
      "\n",
      "Epoch [29/100] Loss: 1.1994238506923884, perplexity: 3.3182045906176363\n",
      "Text generated after epoch 28 :\n",
      "\n",
      "How are the and from there lazy driving mexico\n",
      "\n",
      "Epoch [30/100] Loss: 1.1651866466760818, perplexity: 3.206521313898552\n",
      "Text generated after epoch 29 :\n",
      "\n",
      "How are is in is rouladen Montargis n.m. kj dot Africa BRANDO\n",
      "\n",
      "Epoch [31/100] Loss: 1.1375308972124558, perplexity: 3.11905757598261\n",
      "Text generated after epoch 30 :\n",
      "\n",
      "How are is in is there Montargis wayne sodium engine there\n",
      "\n",
      "Epoch [32/100] Loss: 1.1006496869140352, perplexity: 3.0061184254559614\n",
      "Text generated after epoch 31 :\n",
      "\n",
      "How are the is from characters complete prince mexico temperature metabolic ranges\n",
      "\n",
      "Epoch [33/100] Loss: 1.0682886577319188, perplexity: 2.910394554501751\n",
      "Text generated after epoch 32 :\n",
      "\n",
      "How are in the of rouladen columbus canadian dukes dot in war\n",
      "\n",
      "Epoch [34/100] Loss: 1.0449181547325181, perplexity: 2.843165814462246\n",
      "Text generated after epoch 33 :\n",
      "\n",
      "How are is in data rouladen jefferson wayne protection cut maine alighieri\n",
      "\n",
      "Epoch [35/100] Loss: 1.0090581996197665, perplexity: 2.743016424212347\n",
      "Text generated after epoch 34 :\n",
      "\n",
      "How are is the data rouladen prince SE protection cut williams\n",
      "\n",
      "Epoch [36/100] Loss: 0.9722503768243131, perplexity: 2.6438875129763115\n",
      "Text generated after epoch 35 :\n",
      "\n",
      "How are the in data specific SE european MOAT scout post\n",
      "\n",
      "Epoch [37/100] Loss: 0.942409362697209, perplexity: 2.566156778417917\n",
      "Text generated after epoch 36 :\n",
      "\n",
      "How are in the assistant rouladen columbus SE grammy dot for nissan\n",
      "\n",
      "Epoch [38/100] Loss: 0.9195833327871479, perplexity: 2.5082450686939537\n",
      "Text generated after epoch 37 :\n",
      "\n",
      "How are in is data specific wayne Montargis MOAT make roles osaka\n",
      "\n",
      "Epoch [39/100] Loss: 0.9003583212359648, perplexity: 2.4604845971018574\n",
      "Text generated after epoch 38 :\n",
      "\n",
      "How are is the of specific sodium lazy fraud to and minka\n",
      "\n",
      "Epoch [40/100] Loss: 0.884391605916378, perplexity: 2.421510710451929\n",
      "Text generated after epoch 39 :\n",
      "\n",
      "How are in the data specific columbus lazy protection start for interstate\n",
      "\n",
      "Epoch [41/100] Loss: 0.8657935980215672, perplexity: 2.3768916341928605\n",
      "Text generated after epoch 40 :\n",
      "\n",
      "How are the is of directions 1967 Montargis mark about Red The\n",
      "\n",
      "Epoch [42/100] Loss: 0.8508038955389493, perplexity: 2.3415284397916856\n",
      "Text generated after epoch 41 :\n",
      "\n",
      "How are in the assistant directions european 1967 flat about are\n",
      "\n",
      "Epoch [43/100] Loss: 0.8395430084285972, perplexity: 2.3153086584361273\n",
      "Text generated after epoch 42 :\n",
      "\n",
      "How are is in assistant directions prince vietnam flat are territories\n",
      "\n",
      "Epoch [44/100] Loss: 0.8204001203323789, perplexity: 2.2714084924555977\n",
      "Text generated after epoch 43 :\n",
      "\n",
      "How are the is assistant characters campaign prince grammy in SLR on\n",
      "\n",
      "Epoch [45/100] Loss: 0.7958737848094464, perplexity: 2.216376787317505\n",
      "Text generated after epoch 44 :\n",
      "\n",
      "How are to is data directions this vitamin protection about for delta\n",
      "\n",
      "Epoch [46/100] Loss: 0.7782761966340105, perplexity: 2.177715075522859\n",
      "Text generated after epoch 45 :\n",
      "\n",
      "How are to is assistant characters hunt vitamin flat temperature red\n",
      "\n",
      "Epoch [47/100] Loss: 0.7622067153757233, perplexity: 2.142999997609019\n",
      "Text generated after epoch 46 :\n",
      "\n",
      "How are to is return jonas get prince snape brothers biltmore\n",
      "\n",
      "Epoch [48/100] Loss: 0.738726924132556, perplexity: 2.093268927315186\n",
      "Text generated after epoch 47 :\n",
      "\n",
      "How are in the assistant characters wayne campaign books\n",
      "\n",
      "Epoch [49/100] Loss: 0.7299020075746497, perplexity: 2.074877275455251\n",
      "Text generated after epoch 48 :\n",
      "\n",
      "How are the is data characters campaign sodium name in\n",
      "\n",
      "Epoch [50/100] Loss: 0.7147772408422837, perplexity: 2.0437313716691863\n",
      "Text generated after epoch 49 :\n",
      "\n",
      "How are in the assistant characters wayne campaign biltmore in roles\n",
      "\n",
      "Epoch [51/100] Loss: 0.6966024974293393, perplexity: 2.0069225867165996\n",
      "Text generated after epoch 50 :\n",
      "\n",
      "How are the is return characters SE enforced address in Desmanthus\n",
      "\n",
      "Epoch [52/100] Loss: 0.6807155575880935, perplexity: 1.975290660875333\n",
      "Text generated after epoch 51 :\n",
      "\n",
      "How are in is of archegonia Texas prince book sophia is when\n",
      "\n",
      "Epoch [53/100] Loss: 0.6700342891356094, perplexity: 1.954304330893295\n",
      "Text generated after epoch 52 :\n",
      "\n",
      "How are the in of jonas SE wayne fraud cutter nissan fontes\n",
      "\n",
      "Epoch [54/100] Loss: 0.6536433073144504, perplexity: 1.92253246152658\n",
      "Text generated after epoch 53 :\n",
      "\n",
      "How are in is assistant archegonia columbus prince Africa sophia athenians sing\n",
      "\n",
      "Epoch [55/100] Loss: 0.6491039791629867, perplexity: 1.913825233236513\n",
      "Text generated after epoch 54 :\n",
      "\n",
      "How are is the of archegonia vitamin campaign john\n",
      "\n",
      "Epoch [56/100] Loss: 0.643016300609725, perplexity: 1.9022098715754066\n",
      "Text generated after epoch 55 :\n",
      "\n",
      "How are the to from directions purdue this general are level\n",
      "\n",
      "Epoch [57/100] Loss: 0.6255457963033099, perplexity: 1.8692659174893569\n",
      "Text generated after epoch 56 :\n",
      "\n",
      "How are in is assistant text wayne prince flat to fontes territories\n",
      "\n",
      "Epoch [58/100] Loss: 0.6174242176459961, perplexity: 1.8541460091803974\n",
      "Text generated after epoch 57 :\n",
      "\n",
      "How are is the from directions prince 1967 general are when\n",
      "\n",
      "Epoch [59/100] Loss: 0.6028206441352727, perplexity: 1.8272656043374398\n",
      "Text generated after epoch 58 :\n",
      "\n",
      "How are to the capacity glacier get canadian is minus known file\n",
      "\n",
      "Epoch [60/100] Loss: 0.5931415745390899, perplexity: 1.8096646910254832\n",
      "Text generated after epoch 59 :\n",
      "\n",
      "How are to the capacity glacier get canadian of caves known citizenship\n",
      "\n",
      "Epoch [61/100] Loss: 0.576470251969817, perplexity: 1.7797452782725107\n",
      "Text generated after epoch 60 :\n",
      "\n",
      "How are to the of text become canadian john can\n",
      "\n",
      "Epoch [62/100] Loss: 0.5710239387125444, perplexity: 1.7700785758085085\n",
      "Text generated after epoch 61 :\n",
      "\n",
      "How are to the will directions get SE ie9 of their nissan\n",
      "\n",
      "Epoch [63/100] Loss: 0.5624546552833242, perplexity: 1.7549750763084229\n",
      "Text generated after epoch 62 :\n",
      "\n",
      "How are the is change directions purpose julia is are is\n",
      "\n",
      "Epoch [64/100] Loss: 0.5500483192681651, perplexity: 1.7333367694081538\n",
      "Text generated after epoch 63 :\n",
      "\n",
      "How are is the change inventor prince canadian john created on from\n",
      "\n",
      "Epoch [65/100] Loss: 0.5399905388911027, perplexity: 1.7159906269338687\n",
      "Text generated after epoch 64 :\n",
      "\n",
      "How are the to capacity inventor SE this a gospel Jackie\n",
      "\n",
      "Epoch [66/100] Loss: 0.5314513345400116, perplexity: 1.7013998179168022\n",
      "Text generated after epoch 65 :\n",
      "\n",
      "How are to is capacity glacier this julia a minus the prognosis\n",
      "\n",
      "Epoch [67/100] Loss: 0.5145644604947791, perplexity: 1.6729097248296383\n",
      "Text generated after epoch 66 :\n",
      "\n",
      "How are the to change directions canadian get over of from\n",
      "\n",
      "Epoch [68/100] Loss: 0.5222368355401182, perplexity: 1.685794280020973\n",
      "Text generated after epoch 67 :\n",
      "\n",
      "How are is to capacity inventor julia become a of prognosis\n",
      "\n",
      "Epoch [69/100] Loss: 0.5006273301041249, perplexity: 1.6497558876758063\n",
      "Text generated after epoch 68 :\n",
      "\n",
      "How are is to capacity inventor 7 become a created implications a\n",
      "\n",
      "Epoch [70/100] Loss: 0.5012877188785564, perplexity: 1.650845727764052\n",
      "Text generated after epoch 69 :\n",
      "\n",
      "How are to is change members this human john of on\n",
      "\n",
      "Epoch [71/100] Loss: 0.48883778848921383, perplexity: 1.6304202252421596\n",
      "Text generated after epoch 70 :\n",
      "\n",
      "How are to is will glacier get 7 happen caves split\n",
      "\n",
      "Epoch [72/100] Loss: 0.4801092750315769, perplexity: 1.6162510084233737\n",
      "Text generated after epoch 71 :\n",
      "\n",
      "How are the is capacity singapore's SE 7 won primary nissan year\n",
      "\n",
      "Epoch [73/100] Loss: 0.4752701879104605, perplexity: 1.6084487221808086\n",
      "Text generated after epoch 72 :\n",
      "\n",
      "How are to is capacity singapore's become 7 won alcohol los picture\n",
      "\n",
      "Epoch [74/100] Loss: 0.4587172491681284, perplexity: 1.5820433154724403\n",
      "Text generated after epoch 73 :\n",
      "\n",
      "How are the to capacity inventor SE become of will Desmanthus\n",
      "\n",
      "Epoch [75/100] Loss: 0.45190296548154846, perplexity: 1.5712994708924681\n",
      "Text generated after epoch 74 :\n",
      "\n",
      "How are to is change glacier get julia over julia their staffing\n",
      "\n",
      "Epoch [76/100] Loss: 0.4481436353906512, perplexity: 1.5654035268533828\n",
      "Text generated after epoch 75 :\n",
      "\n",
      "How are the is of singapore's purpose vitamin book led is a\n",
      "\n",
      "Epoch [77/100] Loss: 0.442476132733828, perplexity: 1.5565566916174916\n",
      "Text generated after epoch 76 :\n",
      "\n",
      "How are the is of singapore's canadian vitamin book members citizenship delta\n",
      "\n",
      "Epoch [78/100] Loss: 0.43327583477641907, perplexity: 1.5423015824453192\n",
      "Text generated after epoch 77 :\n",
      "\n",
      "How are to the of sam's get SE book father known Alaska\n",
      "\n",
      "Epoch [79/100] Loss: 0.4212935354393585, perplexity: 1.523931540676224\n",
      "Text generated after epoch 78 :\n",
      "\n",
      "How are the to of sam's purpose become book cowboy in a\n",
      "\n",
      "Epoch [80/100] Loss: 0.41300403655352563, perplexity: 1.5113511265713995\n",
      "Text generated after epoch 79 :\n",
      "\n",
      "How are the is capacity inventor SE vitamin of will\n",
      "\n",
      "Epoch [81/100] Loss: 0.4155855267942804, perplexity: 1.5152577049825358\n",
      "Text generated after epoch 80 :\n",
      "\n",
      "How are the is capacity section movie vitamin of related The d\n",
      "\n",
      "Epoch [82/100] Loss: 0.40440938103839896, perplexity: 1.4984172450209963\n",
      "Text generated after epoch 81 :\n",
      "\n",
      "How are the to capacity boys SE become a ave nissan\n",
      "\n",
      "Epoch [83/100] Loss: 0.3914985980925039, perplexity: 1.479195853810764\n",
      "Text generated after epoch 82 :\n",
      "\n",
      "How are the is from section SE julia general related nissan prognosis\n",
      "\n",
      "Epoch [84/100] Loss: 0.38907124332683074, perplexity: 1.4756096749306788\n",
      "Text generated after epoch 83 :\n",
      "\n",
      "How are the to will inventor US this of religions post\n",
      "\n",
      "Epoch [85/100] Loss: 0.3886882977026475, perplexity: 1.4750447048459767\n",
      "Text generated after epoch 84 :\n",
      "\n",
      "How are is to change inventor vitamin get over religions d jewish\n",
      "\n",
      "Epoch [86/100] Loss: 0.37296581866809175, perplexity: 1.452034706504268\n",
      "Text generated after epoch 85 :\n",
      "\n",
      "How are is the change sam's vitamin US is father\n",
      "\n",
      "Epoch [87/100] Loss: 0.36693942175276584, perplexity: 1.443310483247559\n",
      "Text generated after epoch 86 :\n",
      "\n",
      "How are is to capacity inventor julia become a WHO staffing\n",
      "\n",
      "Epoch [88/100] Loss: 0.36566158854207453, perplexity: 1.441467351037299\n",
      "Text generated after epoch 87 :\n",
      "\n",
      "How are the is capacity glacier SE julia won sims Jackie staffing\n",
      "\n",
      "Epoch [89/100] Loss: 0.3688299220839053, perplexity: 1.4460416430098646\n",
      "Text generated after epoch 88 :\n",
      "\n",
      "How are to is change fastest this vitamin john article for\n",
      "\n",
      "Epoch [90/100] Loss: 0.3606443265834186, perplexity: 1.4342532443970373\n",
      "Text generated after epoch 89 :\n",
      "\n",
      "How are the to capacity section purpose this of related is on\n",
      "\n",
      "Epoch [91/100] Loss: 0.3559393358067666, perplexity: 1.4275209462305234\n",
      "Text generated after epoch 90 :\n",
      "\n",
      "How are the is capacity section US julia of paste dot prognosis\n",
      "\n",
      "Epoch [92/100] Loss: 0.3484193539404984, perplexity: 1.4168262768597142\n",
      "Text generated after epoch 91 :\n",
      "\n",
      "How are is to change inventor vitamin buy atomic plays on park\n",
      "\n",
      "Epoch [93/100] Loss: 0.33841630762636343, perplexity: 1.4027243467108994\n",
      "Text generated after epoch 92 :\n",
      "\n",
      "How are is the change fastest vitamin US over amy on post\n",
      "\n",
      "Epoch [94/100] Loss: 0.33689048201974137, perplexity: 1.4005856660254623\n",
      "Text generated after epoch 93 :\n",
      "\n",
      "How are the to will section purpose buy happen paste is\n",
      "\n",
      "Epoch [95/100] Loss: 0.3334356264691998, perplexity: 1.3957551939595192\n",
      "Text generated after epoch 94 :\n",
      "\n",
      "How are the is capacity section carrier vitamin of paste long\n",
      "\n",
      "Epoch [96/100] Loss: 0.32470845175324753, perplexity: 1.3836271930883135\n",
      "Text generated after epoch 95 :\n",
      "\n",
      "How are the to will sam's movie become transfusion father The a\n",
      "\n",
      "Epoch [97/100] Loss: 0.31913326395230435, perplexity: 1.375934675181704\n",
      "Text generated after epoch 96 :\n",
      "\n",
      "How are to the will section get society happen paste biltmore hunt\n",
      "\n",
      "Epoch [98/100] Loss: 0.3213891526510772, perplexity: 1.3790421343889196\n",
      "Text generated after epoch 97 :\n",
      "\n",
      "How are the to will inventor disk get transfusion of oil split\n",
      "\n",
      "Epoch [99/100] Loss: 0.3159682290024209, perplexity: 1.3715866782567636\n",
      "Text generated after epoch 98 :\n",
      "\n",
      "How are to the capacity fastest control 1967 won amy marvin Red\n",
      "\n",
      "Epoch [100/100] Loss: 0.3108002463026148, perplexity: 1.3645166267198607\n",
      "Text generated after epoch 99 :\n",
      "\n",
      "How are to is capacity members get 7 won who their year\n",
      "\n",
      "Loss on train data:  0.3108002463026148\n",
      "Perplexity on train data:  1.3645166267198607\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    seq_len=5\n",
    "    loss=main()\n",
    "    \n",
    "    print(\"Loss on train data: \", loss)\n",
    "    print(\"Perplexity on train data: \", np.exp(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85c755-c0b2-42c0-b7a2-d899b976cad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
